# ðŸ“ˆ Yahoo Finance Data Engineering Pipeline

An end-to-end data engineering project built with free tools to ingest, process, and store financial market data from Yahoo Finance.  
The pipeline supports both **batch ETL** and **streaming ingestion** via Apache Kafka, showcasing scalable and modern data engineering practices.

---

## ðŸŽ¯ Objectives
- Build a **daily batch ETL pipeline** with Airflow, PySpark, and Parquet.
- Extend with **real-time ingestion** of stock ticks through Kafka.
- Demonstrate **scalability** from 100 â†’ 1000+ tickers.
- Showcase both **data engineering skills** and a clear **learning journey**.

---

## ðŸ”§ Tech Stack
- **Python 3.x** â€“ core ETL and utilities.
- **Apache Airflow** â€“ batch orchestration & scheduling.
- **Apache Kafka** â€“ streaming ingestion and message queue.
- **PySpark** â€“ distributed processing of large datasets.
- **Pandas** â€“ lightweight transforms & exploration.
- **Docker Compose** â€“ reproducible local dev stack (Airflow + Kafka + MinIO).
- **MinIO / Local FS** â€“ object storage (S3-compatible).
- **GitHub Actions** â€“ CI for linting & testing.
- **Parquet** â€“ columnar storage format.

---

## ðŸ§  Why These Technologies?
- **Airflow** â†’ robust batch scheduling, retries, DAG-based orchestration.  
- **Kafka** â†’ real-time streaming of stock events into the pipeline.  
- **PySpark** â†’ distributed processing when scaling beyond pandas.  
- **Parquet** â†’ compressed, analytical storage.  
- **Docker** â†’ unified stack with Airflow, Kafka, and Spark.  
- **GitHub Actions** â†’ free CI automation for public repos.  

---
