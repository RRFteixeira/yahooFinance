# ğŸ“ˆ Yahoo Finance Data Engineering Pipeline

An end-to-end data engineering project built with free tools to ingest, process, and store financial market data from Yahoo Finance.  
The pipeline supports both **batch ETL** and **streaming ingestion** via Apache Kafka, showcasing scalable and modern data engineering practices.

---

## âš ï¸ Disclaimer
This project is a **work in progress** and is still under active development.  

It was **partially developed with the assistance of AI tools** to accelerate learning, design, and implementation.  
The main purpose of this repository is to **practice data engineering skills**, showcase a modern stack, and document my learning journey.


---

## ğŸ¯ Objectives
- Build a **daily batch ETL pipeline** with Airflow, PySpark, and Parquet.
- Extend with **real-time ingestion** of stock ticks through Kafka.
- Demonstrate **scalability** from 100 â†’ 1000+ tickers.
- Showcase both **data engineering skills** and a clear **learning journey**.

---

## ğŸ”§ Tech Stack
- **Python 3.x** â€“ core ETL and utilities.
- **Apache Airflow** â€“ batch orchestration & scheduling.
- **Apache Kafka** â€“ streaming ingestion and message queue.
- **PySpark** â€“ distributed processing of large datasets.
- **Pandas** â€“ lightweight transforms & exploration.
- **Docker Compose** â€“ reproducible local dev stack (Airflow + Kafka + MinIO).
- **MinIO / Local FS** â€“ object storage (S3-compatible).
- **GitHub Actions** â€“ CI for linting & testing.
- **Parquet** â€“ columnar storage format.

---

## ğŸ§  Why These Technologies?
- **Airflow** â†’ robust batch scheduling, retries, DAG-based orchestration.  
- **Kafka** â†’ real-time streaming of stock events into the pipeline.  
- **PySpark** â†’ distributed processing when scaling beyond pandas.  
- **Parquet** â†’ compressed, analytical storage.  
- **Docker** â†’ unified stack with Airflow, Kafka, and Spark.  
- **GitHub Actions** â†’ free CI automation for public repos.  

---

## âœ… Batch vs Streaming

### ğŸ“¦ Batch Mode (ETL)
- Triggered daily with **Airflow**  
- Fetches historical **1-min stock bars**  
- Processes with **PySpark**  
- Stores results in **Parquet**

### âš¡ Streaming Mode (Kafka)
- `kafka_producer.py` pushes stock ticks (simulated or via yfinance polling)  
- `kafka_consumer.py` consumes events and writes to **Bronze (raw) storage**  
- Later transformations (**Silver/Gold layers**) handled in **Spark**

---

## ğŸ—ºï¸ Roadmap
- **Phase 0:** Repo scaffolding, Docker stack. 
- **Phase 1:** Batch ingestion with Airflow + pandas. 
- **Phase 2:** Transformations in PySpark. 
- **Phase 3:** Add Kafka producer/consumer for real-time ingestion. 
- **Phase 4:** Data quality checks (Great Expectations). 
- **Phase 5:** Cloud-ready infra with Terraform + AWS S3. 

---

## ğŸ™‹â€â™‚ï¸ What I Learned
- Orchestration with **Airflow** vs event-driven streaming with **Kafka**  
- Designing **idempotent batch jobs** and **at-least-once streaming consumers**  
- Balancing **pandas for prototyping** vs **PySpark for scale**  
- Structuring a **`src/` project layout** for clean imports and testing  

---
